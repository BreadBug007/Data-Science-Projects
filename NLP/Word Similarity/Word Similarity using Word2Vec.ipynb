{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Similarity using Word2Vec (NLP).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDBfb6xPPStn"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('all')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F_vVkwxNlnP"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup as bs\r\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqAkqHxNNy24"
      },
      "source": [
        "## Getting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "696ldbWWN4Ra"
      },
      "source": [
        "resp = requests.get('https://en.wikipedia.org/wiki/Artificial_intelligence')\r\n",
        "parsed_article = bs(resp.text, 'lxml')\r\n",
        "paragraphs = parsed_article.find_all('p')\r\n",
        "\r\n",
        "article_text = \"\"\r\n",
        "\r\n",
        "for p in paragraphs:\r\n",
        "  article_text += p.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlbF8eUQODPt"
      },
      "source": [
        "## Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmQ2WWJHOFN5"
      },
      "source": [
        "def clean_data(text):\r\n",
        "  processed_text = text.lower()\r\n",
        "  processed_text = re.sub('[^a-zA-Z]', ' ', processed_text)\r\n",
        "  processed_text = re.sub(r'\\s+', ' ', processed_text)\r\n",
        "  return processed_text\r\n",
        "\r\n",
        "\r\n",
        "def tokenize_data(text):\r\n",
        "  # Preparing the dataset\r\n",
        "  all_sentences = nltk.sent_tokenize(text)\r\n",
        "  all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\r\n",
        "  return all_words\r\n",
        "\r\n",
        "\r\n",
        "def removing_stopwords(all_words):\r\n",
        "  from nltk.corpus import stopwords\r\n",
        "  for i in range(len(all_words)):\r\n",
        "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]\r\n",
        "  return all_words\r\n",
        "\r\n",
        "\r\n",
        "def prepare_data(text):\r\n",
        "  clean_text = clean_data(text)\r\n",
        "  tokens = tokenize_data(clean_text)\r\n",
        "  processed_tokens = removing_stopwords(tokens)\r\n",
        "  return processed_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPA3SKTJOrzr"
      },
      "source": [
        "all_words = prepare_data(article_text)\r\n",
        "len(all_words[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzFpslUBPQZI"
      },
      "source": [
        "## Building word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JVLmiEAQMTc"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "\r\n",
        "model = Word2Vec(all_words, \r\n",
        "                 min_count = 2,\r\n",
        "                 window = 8,\r\n",
        "                #  negative = 10, # for negative sampling\r\n",
        "                 seed = 14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW7G9zc0R2JN"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1uJb627QNdU"
      },
      "source": [
        "vocabulary = model.wv.vocab\r\n",
        "print(vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBGmOeWLQPRy"
      },
      "source": [
        "# Finding Vectors for a Word\r\n",
        "v1 = model.wv['artificial']\r\n",
        "v1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xoJ-pYFQkWf"
      },
      "source": [
        "## Finding Similar Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAC7k1ywQtlQ"
      },
      "source": [
        "sim_words = model.wv.most_similar('intelligence')\r\n",
        "sim_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF0PHQjnQwUI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}