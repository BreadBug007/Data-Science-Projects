{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDBfb6xPPStn"
   },
   "outputs": [],
   "source": [
    "import nltk\r\n",
    "nltk.download('all')\r\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4F_vVkwxNlnP"
   },
   "outputs": [],
   "source": [
    "import re\r\n",
    "import requests\r\n",
    "from bs4 import BeautifulSoup as bs\r\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqAkqHxNNy24"
   },
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "696ldbWWN4Ra"
   },
   "outputs": [],
   "source": [
    "resp = requests.get('https://en.wikipedia.org/wiki/Artificial_intelligence')\r\n",
    "parsed_article = bs(resp.text, 'lxml')\r\n",
    "paragraphs = parsed_article.find_all('p')\r\n",
    "\r\n",
    "article_text = \"\"\r\n",
    "\r\n",
    "for p in paragraphs:\r\n",
    "  article_text += p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlbF8eUQODPt"
   },
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmQ2WWJHOFN5"
   },
   "outputs": [],
   "source": [
    "def clean_data(text):\r\n",
    "  processed_text = text.lower()\r\n",
    "  processed_text = re.sub('[^a-zA-Z]', ' ', processed_text)\r\n",
    "  processed_text = re.sub(r'\\s+', ' ', processed_text)\r\n",
    "  return processed_text\r\n",
    "\r\n",
    "\r\n",
    "def tokenize_data(text):\r\n",
    "  # Preparing the dataset\r\n",
    "  all_sentences = nltk.sent_tokenize(text)\r\n",
    "  all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\r\n",
    "  return all_words\r\n",
    "\r\n",
    "\r\n",
    "def removing_stopwords(all_words):\r\n",
    "  from nltk.corpus import stopwords\r\n",
    "  for i in range(len(all_words)):\r\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]\r\n",
    "  return all_words\r\n",
    "\r\n",
    "\r\n",
    "def prepare_data(text):\r\n",
    "  clean_text = clean_data(text)\r\n",
    "  tokens = tokenize_data(clean_text)\r\n",
    "  processed_tokens = removing_stopwords(tokens)\r\n",
    "  return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RPA3SKTJOrzr",
    "outputId": "14e333e9-ec79-4d49-e554-05fe7353bb21"
   },
   "outputs": [],
   "source": [
    "all_words = prepare_data(article_text)\r\n",
    "len(all_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzFpslUBPQZI"
   },
   "source": [
    "## Building word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JVLmiEAQMTc"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\r\n",
    "\r\n",
    "model = Word2Vec(all_words, \r\n",
    "                 min_count = 2,\r\n",
    "                 window = 8,\r\n",
    "                #  negative = 10, # for negative sampling\r\n",
    "                 seed = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XW7G9zc0R2JN",
    "outputId": "19c8743c-1fc2-4fd1-f34c-92a4950d19d9"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1uJb627QNdU",
    "outputId": "1c023f8e-f885-47b7-96dc-c0f5c73b1f2d"
   },
   "outputs": [],
   "source": [
    "vocabulary = model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBGmOeWLQPRy",
    "outputId": "a5623760-14e2-4340-9718-09e64ce6c2e0"
   },
   "outputs": [],
   "source": [
    "# Finding Vectors for a Word\r\n",
    "v1 = model.wv['artificial']\r\n",
    "v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xoJ-pYFQkWf"
   },
   "source": [
    "## Finding Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAC7k1ywQtlQ",
    "outputId": "b326a9cf-4926-4a44-ecf4-19225076ee5a"
   },
   "outputs": [],
   "source": [
    "sim_words = model.wv.most_similar('intelligence')\r\n",
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KF0PHQjnQwUI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Word Similarity using Word2Vec (NLP).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
