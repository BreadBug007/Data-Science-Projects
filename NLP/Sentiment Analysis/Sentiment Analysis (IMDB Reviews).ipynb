{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis (IMDB Reviews) (NLP).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Vs2MrxD_5QZX",
        "cp7b3oMPaxgv",
        "tCnLiuTNU_Kz",
        "Fl5aYNiga8m9",
        "HOsB6wmgT-4-",
        "Mu1vFXYTdkfY",
        "hHCO5QTFg1Rq",
        "Jf5AT--hMV7G",
        "Rhgj9DNWTNlT",
        "PjTI9wGoVkvr",
        "yW4m3bLDWB9M"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcsZjaQD5WfP"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAthq2n-2qto"
      },
      "source": [
        "import os\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWHYRVpRVHbW"
      },
      "source": [
        "drive_dir = \"/content/drive/MyDrive/Internship Work/Sentiment Analysis (IMDB)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md9_viF7wEIj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgaYhCX0UNXA"
      },
      "source": [
        "# Reading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znb2SfMwV9Ri"
      },
      "source": [
        "Dataset can be obtained from https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BejBqyna3QdS"
      },
      "source": [
        "imdb_data = pd.read_csv(os.path.join(drive_dir, 'IMDB Dataset.csv'))\r\n",
        "imdb_data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2Wo5VZg4gmq"
      },
      "source": [
        "imdb_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA2P3bSk4hlb"
      },
      "source": [
        "# summary\r\n",
        "imdb_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgBp5I3_5Lqg"
      },
      "source": [
        "# value counts\r\n",
        "imdb_data['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs2MrxD_5QZX"
      },
      "source": [
        "# Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFPIrSVj6Omg"
      },
      "source": [
        "import re\r\n",
        "import string\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCUlyCnXJABk"
      },
      "source": [
        "tokenizer = ToktokTokenizer()\r\n",
        "ps = nltk.porter.PorterStemmer()\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "stopword_list = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTeuhlLfJO9o"
      },
      "source": [
        "stop = set(stopwords.words('english'))\r\n",
        "print(stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RwDT0eN6POw"
      },
      "source": [
        "# Removing the html strips\r\n",
        "def strip_html(text):\r\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\r\n",
        "  return soup.get_text().lower()\r\n",
        "    \r\n",
        "\r\n",
        "# removing punctuation and non-alphabetic tokens\r\n",
        "def clean_text(text):\r\n",
        "  text = strip_html(text)\r\n",
        "  # split sentences on dots and make a single sentence\r\n",
        "  text = \" \".join(text.split('.'))\r\n",
        "  # split into words\r\n",
        "  tokens = tokenizer.tokenize(text)\r\n",
        "  # remove punctuation from each word\r\n",
        "  table = str.maketrans('', '', string.punctuation)\r\n",
        "  stripped = (w.translate(table) for w in tokens)\r\n",
        "  # remove remaining tokens that are not alphabetic\r\n",
        "  words = (word for word in stripped if word.isalpha())\r\n",
        "  return words\r\n",
        "\r\n",
        "\r\n",
        "def get_wordnet_pos(text):\r\n",
        "  \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\r\n",
        "  tag = nltk.pos_tag([text])[0][1][0].upper()\r\n",
        "  tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "              \"N\": wordnet.NOUN,\r\n",
        "              \"V\": wordnet.VERB,\r\n",
        "              \"R\": wordnet.ADV}\r\n",
        "\r\n",
        "  return tag_dict.get(tag, wordnet.NOUN)\r\n",
        "\r\n",
        "\r\n",
        "def lemmatize_with_pos(tokens):\r\n",
        "  tokens = (lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens)\r\n",
        "  return \" \".join(tokens)\r\n",
        "\r\n",
        "\r\n",
        "def simple_stemmer(tokens):\r\n",
        "  tokens = (ps.stem(word) for word in tokens)\r\n",
        "  return \" \".join(tokens)\r\n",
        "\r\n",
        "\r\n",
        "#removing the stopwords\r\n",
        "def remove_stopwords(tokens):\r\n",
        "  filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\r\n",
        "  # filtered_text = ' '.join(filtered_tokens)\r\n",
        "  return filtered_tokens\r\n",
        "\r\n",
        "\r\n",
        "def prepare_data(text):\r\n",
        "  text = strip_html(text)\r\n",
        "  tokens = clean_text(text)\r\n",
        "  filtered_tokens = remove_stopwords(tokens)\r\n",
        "  return \" \".join(filtered_tokens)\r\n",
        "\r\n",
        "\r\n",
        "def prepare_data_using_stemming(text):\r\n",
        "  text = strip_html(text)\r\n",
        "  tokens = clean_text(text)\r\n",
        "  filtered_tokens = remove_stopwords(tokens)\r\n",
        "  stemmed_text = simple_stemmer(filtered_tokens)\r\n",
        "  return stemmed_text\r\n",
        "\r\n",
        "\r\n",
        "def prepare_data_using_lemmatization(text):\r\n",
        "  text = strip_html(text)\r\n",
        "  tokens = clean_text(text)\r\n",
        "  filtered_tokens = remove_stopwords(tokens)\r\n",
        "  lemmatized_text = lemmatize_with_pos(filtered_tokens)\r\n",
        "  return lemmatized_text\r\n",
        "\r\n",
        "\r\n",
        "# Apply function on review column\r\n",
        "\r\n",
        "start = time.time()\r\n",
        "if os.path.exists(os.path.join(drive_dir, \"IMDB Dataset Clean.csv\")):\r\n",
        "  print(\"Dataset exists\")\r\n",
        "  imdb_data_clean = pd.read_csv(os.path.join(drive_dir, \"IMDB Dataset Clean.csv\"))\r\n",
        "else:\r\n",
        "  print(\"Starting cleaning\")\r\n",
        "  imdb_data_clean = imdb_data.copy()\r\n",
        "  imdb_data_clean['review'] = imdb_data_clean['review'].apply(prepare_data)\r\n",
        "print(\"Finished in\", time.time() - start)\r\n",
        "\r\n",
        "# start = time.time()\r\n",
        "# if os.path.exists(os.path.join(drive_dir, \"IMDB Dataset Clean (Stemmed).csv\")):\r\n",
        "#   print(\"Stemmed dataset exists\")\r\n",
        "#   imdb_data_stem = pd.read_csv(os.path.join(drive_dir, \"IMDB Dataset Clean (Stemmed).csv\"))\r\n",
        "# else:\r\n",
        "#   print(\"Starting cleaning with stemming\")\r\n",
        "#   imdb_data_stem = imdb_data.copy()\r\n",
        "#   imdb_data_stem['review'] = imdb_data_stem['review'].apply(prepare_data_using_stemming)\r\n",
        "# print(\"Finished in\", time.time() - start)\r\n",
        "\r\n",
        "\r\n",
        "# start = time.time()\r\n",
        "# if os.path.exists(os.path.join(drive_dir, \"IMDB Dataset Clean (Lemmatized).csv\")):\r\n",
        "#   print(\"Lemmatized dataset exists\")\r\n",
        "#   imdb_data_lemma = pd.read_csv(os.path.join(drive_dir, \"IMDB Dataset Clean (Lemmatized).csv\"))\r\n",
        "# else:\r\n",
        "#   print(\"Starting cleaning with lemmatization\")\r\n",
        "#   imdb_data_lemma = imdb_data.copy()\r\n",
        "#   imdb_data_lemma['review'] = imdb_data_lemma['review'].apply(prepare_data_using_lemmatization)\r\n",
        "# print(\"Finished in\", time.time() - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBYVToJg1eBR"
      },
      "source": [
        "# imdb_data_clean.to_csv(os.path.join(drive_dir, \"IMDB Dataset Clean.csv\"), index=False)\r\n",
        "# imdb_data_stem.to_csv(os.path.join(drive_dir, \"IMDB Dataset Clean (Stemmed).csv\"), index=False)\r\n",
        "# imdb_data_lemma.to_csv(os.path.join(drive_dir, \"IMDB Dataset Clean (Lemmatized).csv\"), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp7b3oMPaxgv"
      },
      "source": [
        "# Machine Learning approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiD0eLr8kz9F"
      },
      "source": [
        "imdb_data_final = imdb_data_clean.copy()\r\n",
        "# imdb_data_final = imdb_data_stem.copy()\r\n",
        "# imdb_data_final = imdb_data_lemma.copy()\r\n",
        "imdb_data_final['review'].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7lBRkKK8uLi"
      },
      "source": [
        "#### Normalizing train reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe75JimsKEnz"
      },
      "source": [
        "norm_train_reviews=imdb_data_final.review[:40000]\r\n",
        "norm_train_reviews[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOMnOQCMKKzT"
      },
      "source": [
        "#### Normalizing test reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daZqR39VKMRa"
      },
      "source": [
        "norm_test_reviews=imdb_data_final.review[40000:]\r\n",
        "norm_test_reviews[45005]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai_9ZSnHKRoW"
      },
      "source": [
        "### Bag of Words model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJYLcaAVKksp"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgOvx37KKXFV"
      },
      "source": [
        "#Count vectorizer for bag of words\r\n",
        "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\r\n",
        "#transformed train reviews\r\n",
        "cv_train_reviews=cv.fit_transform(norm_train_reviews)\r\n",
        "#transformed test reviews\r\n",
        "cv_test_reviews=cv.transform(norm_test_reviews)\r\n",
        "\r\n",
        "print('BOW_cv_train:',cv_train_reviews.shape)\r\n",
        "print('BOW_cv_test:',cv_test_reviews.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrxPsz2JKls9"
      },
      "source": [
        "#### Term Frequency-Inverse Document Frequency model (TFIDF) model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b5ah7VzPFcZ"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1O65_axPz6E"
      },
      "source": [
        "#Tfidf vectorizer\r\n",
        "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\r\n",
        "#transformed train reviews\r\n",
        "tv_train_reviews=tv.fit_transform(norm_train_reviews)\r\n",
        "#transformed test reviews\r\n",
        "tv_test_reviews=tv.transform(norm_test_reviews)\r\n",
        "print('Tfidf_train:',tv_train_reviews.shape)\r\n",
        "print('Tfidf_test:',tv_test_reviews.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7-fapwIQew-"
      },
      "source": [
        "#### Labeling the sentiment text\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MopIsQENRn2M"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFcMrR4vR2LD"
      },
      "source": [
        "#labeling the sentiment data\r\n",
        "lb=LabelBinarizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLgb-YeeRvKX"
      },
      "source": [
        "#transformed sentiment data\r\n",
        "sentiment_data=lb.fit_transform(imdb_data_final['sentiment'])\r\n",
        "print(sentiment_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzoTVr0ORzxH"
      },
      "source": [
        "\r\n",
        "\r\n",
        "#### Split the sentiment data\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3QodyENU8mP"
      },
      "source": [
        "#Spliting the sentiment data\r\n",
        "train_sentiments=sentiment_data[:40000]\r\n",
        "test_sentiments=sentiment_data[40000:]\r\n",
        "print(train_sentiments)\r\n",
        "print(test_sentiments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCnLiuTNU_Kz"
      },
      "source": [
        "## Building models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0gGlMvGMgmD"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7Pz48psXuI1"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40BFbN4RXeSQ"
      },
      "source": [
        "Logistic regression model for bag of words features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lemflg-VIkN"
      },
      "source": [
        "#training the model\r\n",
        "lr = LogisticRegression(penalty='l2',\r\n",
        "                        max_iter=500,\r\n",
        "                        C=1,\r\n",
        "                        random_state=42)\r\n",
        "#Fitting the model for Bag of words\r\n",
        "lr_bow = lr.fit(cv_train_reviews, np.ravel(train_sentiments))\r\n",
        "print(lr_bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg_YEURXXqkc"
      },
      "source": [
        "Logistic regression model for TF-IDF features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe7v-kk1on8h"
      },
      "source": [
        "#training the model\r\n",
        "lr = LogisticRegression(penalty='l2',\r\n",
        "                        max_iter=500,\r\n",
        "                        C=1,\r\n",
        "                        random_state=42)\r\n",
        "#Fitting the model for tfidf features\r\n",
        "lr_tfidf = lr.fit(tv_train_reviews, np.ravel(train_sentiments))\r\n",
        "print(lr_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzitj_cUoyvN"
      },
      "source": [
        "\r\n",
        "\r\n",
        "Logistic regression model performane on test dataset\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDsXAirwpAm4"
      },
      "source": [
        "#Predicting the model for bag of words\r\n",
        "lr_bow_predict=lr.predict(cv_test_reviews)\r\n",
        "print(\"lr_bow_pred\", lr_bow_predict)\r\n",
        "\r\n",
        "#Predicting the model for tfidf features\r\n",
        "lr_tfidf_predict=lr.predict(tv_test_reviews)\r\n",
        "print(\"lr_tfidf_pred\", lr_tfidf_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPfRcjmPpCjY"
      },
      "source": [
        "\r\n",
        "\r\n",
        "Accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KVwFxggpFS1"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYBjBxYRpHN8"
      },
      "source": [
        "#Accuracy score for bag of words\r\n",
        "lr_bow_score=accuracy_score(test_sentiments,\r\n",
        "                            lr_bow_predict)\r\n",
        "print(\"lr_bow_score :\",lr_bow_score)\r\n",
        "\r\n",
        "#Accuracy score for tfidf features\r\n",
        "lr_tfidf_score=accuracy_score(test_sentiments,\r\n",
        "                              lr_tfidf_predict)\r\n",
        "print(\"lr_tfidf_score :\",lr_tfidf_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5raMyoLpTYy"
      },
      "source": [
        "\r\n",
        "\r\n",
        "Print the classification report\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz84STd5pZ7p"
      },
      "source": [
        "#Classification report for bag of words \r\n",
        "lr_bow_report=classification_report(test_sentiments,\r\n",
        "                                    lr_bow_predict,\r\n",
        "                                    target_names=['Positive','Negative'])\r\n",
        "print(lr_bow_report)\r\n",
        "\r\n",
        "#Classification report for tfidf features\r\n",
        "lr_tfidf_report=classification_report(test_sentiments,\r\n",
        "                                      lr_tfidf_predict,\r\n",
        "                                      target_names=['Positive','Negative'])\r\n",
        "print(lr_tfidf_report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXtppBRppkoc"
      },
      "source": [
        "\r\n",
        "\r\n",
        "Confusion matrix\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49GfuaoJpqsK"
      },
      "source": [
        "#confusion matrix for bag of words\r\n",
        "cm_bow=confusion_matrix(test_sentiments,lr_bow_predict,labels=[1,0])\r\n",
        "print(cm_bow)\r\n",
        "\r\n",
        "#confusion matrix for tfidf features\r\n",
        "cm_tfidf=confusion_matrix(test_sentiments,lr_tfidf_predict,labels=[1,0])\r\n",
        "print(cm_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl5aYNiga8m9"
      },
      "source": [
        "# Deep Learning approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOsB6wmgT-4-"
      },
      "source": [
        "### Initiate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5gdQ3KLa_F1"
      },
      "source": [
        "dataset = imdb_data_clean.copy()\r\n",
        "dataset.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZNeTy0qbDUC"
      },
      "source": [
        "train_docs=dataset.review[:40000]\r\n",
        "train_docs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1lOkPeMdMOO"
      },
      "source": [
        "test_docs=dataset.review[40000:]\r\n",
        "test_docs[45005]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu1vFXYTdkfY"
      },
      "source": [
        "### Define a Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLp0B7MFdrvW"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4eUV7Ftd17O"
      },
      "source": [
        "def add_doc_to_vocab(text, vocab):\r\n",
        "  tokens = text.split()\r\n",
        "  # update counts \r\n",
        "  vocab.update(tokens)\r\n",
        "\r\n",
        "\r\n",
        "def process_docs(dataset, vocab):\r\n",
        "  for row in dataset:\r\n",
        "    add_doc_to_vocab(row, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtNLd0rredta"
      },
      "source": [
        "# define vocab\r\n",
        "vocab = Counter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DllX7g_QeeS0"
      },
      "source": [
        "process_docs(train_docs, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9zZLjfXe4xi"
      },
      "source": [
        "print(len(vocab))\r\n",
        "print(vocab.most_common(50))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLRr8liIfaX2"
      },
      "source": [
        "# keep tokens with a min occurrence\r\n",
        "min_occurance = 2\r\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurance]\r\n",
        "print(len(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2a125BpgmcX"
      },
      "source": [
        "# save list to file\r\n",
        "def save_list(lines, filename):\r\n",
        "  # convert lines to a single blob of text\r\n",
        "  data = '\\n'.join(lines)\r\n",
        "  # open file\r\n",
        "  file = open(filename, 'w')\r\n",
        "  # write text\r\n",
        "  file.write(data)\r\n",
        "  # close file\r\n",
        "  file.close()\r\n",
        "\r\n",
        "# save tokens to a vocabulary file\r\n",
        "save_list(tokens, os.path.join(drive_dir, 'vocab.txt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHCO5QTFg1Rq"
      },
      "source": [
        "### Train Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJb-7akUhVAL"
      },
      "source": [
        "# load doc into memory\r\n",
        "def load_doc(filename):\r\n",
        "\t# open the file as read only\r\n",
        "\tfile = open(filename, 'r')\r\n",
        "\t# read all text\r\n",
        "\ttext = file.read()\r\n",
        "\t# close the file\r\n",
        "\tfile.close()\r\n",
        "\treturn text\r\n",
        "\r\n",
        "# load the vocabulary\r\n",
        "vocab_filename = os.path.join(drive_dir, 'vocab.txt')\r\n",
        "vocab = load_doc(vocab_filename)\r\n",
        "vocab = vocab.split()\r\n",
        "vocab = set(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-wXIcD7o_5h"
      },
      "source": [
        "Encoding each document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVS4ELrehhMB"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKpv-KV6nRQZ"
      },
      "source": [
        "# create the tokenizer\r\n",
        "tokenizer = Tokenizer(num_words=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzWW_WBWnUqK"
      },
      "source": [
        "# fit the tokenizer on the documents\r\n",
        "tokenizer.fit_on_texts(train_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWZ1M-hAnX-U"
      },
      "source": [
        "# sequence encode\r\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXNw1znopDrA"
      },
      "source": [
        "Padding documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X05Q20yWoxX6"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59yfSb83pHpQ"
      },
      "source": [
        "# pad sequences\r\n",
        "# max_length = max([len(s.split()) for s in train_docs])\r\n",
        "max_length = 100\r\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7c3GQXvpLLZ"
      },
      "source": [
        "# define training labels\r\n",
        "ytrain = dataset.sentiment[:40000].map({\"positive\": 1, \"negative\": 0}).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpT3c2eipask"
      },
      "source": [
        "# define testing data\r\n",
        "# sequence encode\r\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\r\n",
        "# pad sequences\r\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\r\n",
        "# define test labels\r\n",
        "ytest = dataset.sentiment[40000:].map({\"positive\": 1, \"negative\": 0}).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-vorp-OT7L6"
      },
      "source": [
        "### Building models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf5AT--hMV7G"
      },
      "source": [
        "#### Initiate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0USePgk7sXm3"
      },
      "source": [
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.models import load_model\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import Embedding\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import GlobalMaxPooling1D\r\n",
        "from keras.layers import Bidirectional\r\n",
        "from keras.layers.convolutional import Conv1D\r\n",
        "from keras.layers.convolutional import MaxPooling1D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM7Fno1YrgM-"
      },
      "source": [
        "# define vocabulary size (largest integer value)\r\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljv-Oj3FMoSN"
      },
      "source": [
        "embeddings_dictionary = dict()\r\n",
        "glove_file = open(os.path.join(drive_dir, 'glove.6B.100d.txt'), encoding=\"utf8\")\r\n",
        "\r\n",
        "for line in glove_file:\r\n",
        "  records = line.split()\r\n",
        "  word = records[0]\r\n",
        "  vector_dimensions = np.asarray(records[1:], dtype='float32')\r\n",
        "  embeddings_dictionary[word] = vector_dimensions\r\n",
        "glove_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVmjmjhxMGf3"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\r\n",
        "for word, index in tokenizer.word_index.items():\r\n",
        "    embedding_vector = embeddings_dictionary.get(word)\r\n",
        "    if embedding_vector is not None:\r\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhgj9DNWTNlT"
      },
      "source": [
        "#### Simple neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgFjNe8BO6SM"
      },
      "source": [
        "use_simple_model = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3jjmRiKTKnA"
      },
      "source": [
        "if os.path.exists(os.path.join(drive_dir, \"model_simple.h5\")) and use_simple_model is True:\r\n",
        "  print(\"Model exists\")\r\n",
        "  model_simple = load_model(os.path.join(drive_dir, \"model_simple.h5\"))\r\n",
        "  print(model_simple.summary())\r\n",
        "  history_simple_dict = pd.read_csv(os.path.join(drive_dir, \"history_simple.csv\")).to_dict('list')\r\n",
        "else:\r\n",
        "  # define model\r\n",
        "  print(\"Training model\")\r\n",
        "  model_simple = Sequential()\r\n",
        "  model_simple.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length , trainable=False))\r\n",
        "  model_simple.add(Flatten())\r\n",
        "  model_simple.add(Dense(1, activation='sigmoid'))\r\n",
        "  # compile model\r\n",
        "  model_simple.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
        "  print(model_simple.summary())\r\n",
        "  # fit model\r\n",
        "  history_simple = model_simple.fit(Xtrain, ytrain, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\r\n",
        "  history_simple_dict = history_simple.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X_uLVWZTkYl"
      },
      "source": [
        "score_simple = model_simple.evaluate(Xtest, ytest, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwatuhhdTppb"
      },
      "source": [
        "print(\"Test Score:\", score_simple[0])\r\n",
        "print(\"Test Accuracy:\", score_simple[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAwZOCO1Ts8l"
      },
      "source": [
        "plt.plot(history_simple_dict['acc'])\r\n",
        "plt.plot(history_simple_dict['val_acc'])\r\n",
        "\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.plot(history_simple_dict['loss'])\r\n",
        "plt.plot(history_simple_dict['val_loss'])\r\n",
        "\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FpVHGt9pQwu"
      },
      "source": [
        "# save model\r\n",
        "model_simple.save(os.path.join(drive_dir, \"model_simple.h5\"))\r\n",
        "# save history data\r\n",
        "pd.DataFrame.from_dict(history_simple_dict).to_csv(os.path.join(drive_dir, 'history_simple.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjTI9wGoVkvr"
      },
      "source": [
        "#### Convolutional neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMk45c89O3yW"
      },
      "source": [
        "use_cnn_model = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mhqA3-zrhZH"
      },
      "source": [
        "# define model\r\n",
        "if os.path.exists(os.path.join(drive_dir, \"model_cnn.h5\")) and use_cnn_model is True:\r\n",
        "  print(\"Model exists\")\r\n",
        "  model_cnn = load_model(os.path.join(drive_dir, \"model_cnn.h5\"))\r\n",
        "  print(model_cnn.summary())\r\n",
        "  history_cnn_dict = pd.read_csv(os.path.join(drive_dir, \"history_cnn.csv\")).to_dict('list')\r\n",
        "else:\r\n",
        "  model_cnn = Sequential()\r\n",
        "  embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\r\n",
        "  model_cnn.add(embedding_layer)\r\n",
        "  model_cnn.add(Conv1D(128, 5, activation='relu'))\r\n",
        "  model_cnn.add(Dropout(0.5))\r\n",
        "  model_cnn.add(GlobalMaxPooling1D())\r\n",
        "  model_cnn.add(Dense(1, activation='sigmoid'))\r\n",
        "  # compile model\r\n",
        "  model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
        "  print(model_cnn.summary())\r\n",
        "  # fit model\r\n",
        "  # es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=1)\r\n",
        "  history_cnn = model_cnn.fit(Xtrain, \r\n",
        "                              ytrain, \r\n",
        "                              batch_size=128, \r\n",
        "                              epochs=6, \r\n",
        "                              verbose=1, \r\n",
        "                              validation_split=0.2, \r\n",
        "                              # callbacks=[es_callback]\r\n",
        "                              )\r\n",
        "  history_cnn_dict = history_cnn.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvbSFiRAslBr"
      },
      "source": [
        "score_cnn = model_cnn.evaluate(Xtest, ytest, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s93zqqulHNJk"
      },
      "source": [
        "print(\"Test Score:\", score_cnn[0])\r\n",
        "print(\"Test Accuracy:\", score_cnn[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbmRSB-pV-n5"
      },
      "source": [
        "plt.plot(history_cnn_dict['acc'])\r\n",
        "plt.plot(history_cnn_dict['val_acc'])\r\n",
        "\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.plot(history_cnn_dict['loss'])\r\n",
        "plt.plot(history_cnn_dict['val_loss'])\r\n",
        "\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDEZwVJmpzgl"
      },
      "source": [
        "# save model\r\n",
        "model_cnn.save(os.path.join(drive_dir, \"model_cnn.h5\"))\r\n",
        "# save history data\r\n",
        "pd.DataFrame.from_dict(history_cnn_dict).to_csv(os.path.join(drive_dir, 'history_cnn.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW4m3bLDWB9M"
      },
      "source": [
        "#### Recurrent neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IS-If1BO-4z"
      },
      "source": [
        "use_rnn_model = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOvBVoUqWhmt"
      },
      "source": [
        "# define model\r\n",
        "if os.path.exists(os.path.join(drive_dir, \"model_rnn.h5\")) and use_rnn_model is True:\r\n",
        "  print(\"Model exists\")\r\n",
        "  model_rnn = load_model(os.path.join(drive_dir, \"model_rnn.h5\"))\r\n",
        "  print(model_rnn.summary())\r\n",
        "  history_rnn_dict = pd.read_csv(os.path.join(drive_dir, \"history_rnn.csv\")).to_dict('list')\r\n",
        "else:\r\n",
        "  model_rnn = Sequential()\r\n",
        "  embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\r\n",
        "  model_rnn.add(embedding_layer)\r\n",
        "  # model_rnn.add(LSTM(128))\r\n",
        "  # model_rnn.add(Bidirectional(LSTM(128, return_sequences=True)))\r\n",
        "  model_rnn.add(Bidirectional(LSTM(128)))\r\n",
        "  model_rnn.add(Dropout(0.5))\r\n",
        "  model_rnn.add(Dense(1, activation='sigmoid'))\r\n",
        "  # compile model\r\n",
        "  model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\r\n",
        "  print(model_rnn.summary())\r\n",
        "  # fit model\r\n",
        "  history_rnn = model_rnn.fit(Xtrain, \r\n",
        "                              ytrain,\r\n",
        "                              batch_size=128,\r\n",
        "                              epochs=6,\r\n",
        "                              verbose=1,\r\n",
        "                              validation_split=0.2)\r\n",
        "  history_rnn_dict = history_rnn.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YScQmEfVXMPo"
      },
      "source": [
        "score_rnn = model_rnn.evaluate(Xtest, ytest, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKDLou5CXPdq"
      },
      "source": [
        "print(\"Test Score:\", score_rnn[0])\r\n",
        "print(\"Test Accuracy:\", score_rnn[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YSaPKEtXRC9"
      },
      "source": [
        "plt.plot(history_rnn_dict['acc'])\r\n",
        "plt.plot(history_rnn_dict['val_acc'])\r\n",
        "\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.plot(history_rnn_dict['loss'])\r\n",
        "plt.plot(history_rnn_dict['val_loss'])\r\n",
        "\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train','test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jclK8bmWp1QZ"
      },
      "source": [
        "# save model\r\n",
        "model_rnn.save(os.path.join(drive_dir, \"model_rnn.h5\"))\r\n",
        "# save history data\r\n",
        "pd.DataFrame.from_dict(history_rnn_dict).to_csv(os.path.join(drive_dir, 'history_rnn.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}